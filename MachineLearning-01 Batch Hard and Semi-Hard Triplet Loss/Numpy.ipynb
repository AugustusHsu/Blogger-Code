{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_triplet(numpy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGGrdX_Owjc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "92268462-0d47-46d8-9075-9bcb7141a197"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "batch = 64\n",
        "emb_dim = 1024\n",
        "\n",
        "np.random.seed(1234)\n",
        "emb1 = np.random.rand(batch,emb_dim).astype(np.float32)\n",
        "np.random.seed(2345)\n",
        "emb2 = np.random.rand(batch,emb_dim).astype(np.float32)\n",
        "emb3 = np.concatenate([emb1, emb2], axis=0)\n",
        "margin = 0.3\n",
        "labels1 = np.arange(batch)\n",
        "labels2 = np.concatenate((labels1,labels1), axis=0)\n",
        "print(emb1.shape)\n",
        "print(emb2.shape)\n",
        "print(emb3.shape)\n",
        "print(labels1.shape)\n",
        "print(labels2.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1024)\n",
            "(64, 1024)\n",
            "(128, 1024)\n",
            "(64,)\n",
            "(128,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIsJ9j9NwmJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def np_distance_metric(embedding, squared=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: float32, with shape [m, d], (batch_size, d)\n",
        "        y: float32, with shape [n, d], (batch_size, d)\n",
        "    Returns:\n",
        "        dist: float32, with shape [m, n], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # |x-y|^2 = x^2 - 2xy + y^2\n",
        "    xy = np.matmul(embedding, np.transpose(embedding))\n",
        "    square_norm = np.diag(xy)\n",
        "    xx = np.expand_dims(square_norm, 0)\n",
        "    yy = np.expand_dims(square_norm, 1)\n",
        "    distances = np.add(xx, yy) - 2.0 * xy\n",
        "    '''\n",
        "    (batch_size,1)-(batch_size,batch_size): Equivalent to each column operation\n",
        "    (batch_size,batch_size)+(1,batch_size): Equivalent to each row operation\n",
        "    '''\n",
        "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
        "    distances = np.maximum(distances, 0.0)\n",
        "    # Get the mask where the zero distances are at.\n",
        "    error_mask = np.less_equal(distances, 0.0).astype(np.float32)\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        distances = np.sqrt(distances + error_mask * 1e-16)\n",
        "\n",
        "    # Undo conditionally adding 1e-16.\n",
        "    distances = np.multiply(distances, np.logical_not(error_mask),)\n",
        "\n",
        "    num_data = np.shape(embedding)[0]\n",
        "    # Explicitly set diagonals to zero.\n",
        "    mask_offdiagonals = np.ones_like(distances) - np.diag(np.ones([num_data]))\n",
        "    distances = np.multiply(distances, mask_offdiagonals)\n",
        "    return distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxWzypKtwqdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def np_masked_minimum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "    Args:\n",
        "      data: float32, with shape [n, m], (batch_size, batch_size)\n",
        "      mask: boolean, with shape [n, m], (batch_size, batch_size)\n",
        "      dim: int, the dimension which want to compute the minimum.\n",
        "    Returns:\n",
        "      masked_minimums: float32, with shape [n, 1], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    axis_maximums = np.max(data, dim, keepdims=True)\n",
        "    masked_minimums = (np.min(np.multiply(data - axis_maximums, mask), dim, keepdims=True) + axis_maximums)\n",
        "    return masked_minimums\n",
        "def np_masked_maximum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
        "    Args:\n",
        "      data: float32, with shape [n, m], (batch_size, batch_size)\n",
        "      mask: boolean, with shape [n, m], (batch_size, batch_size)\n",
        "      dim: int, the dimension over which to compute the maximum.\n",
        "    Returns:\n",
        "      masked_minimums: float32, with shape [n, 1], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    axis_minimums = np.min(data, dim, keepdims=True)\n",
        "    masked_maximums = (np.max(np.multiply(data - axis_minimums, mask), dim, keepdims=True) + axis_minimums)\n",
        "    return masked_maximums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh6lgbGOwq0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "batch hard triplet loss of a batch\n",
        "------------------------------------\n",
        "Args:\n",
        "    labels:     Label Data, shape = (batch_size,1)\n",
        "    embedding:  embedding vector, shape = (batch_size, vector_size)\n",
        "    margin:     margin, scalar\n",
        "    soft::     \tuse log1p or not, boolean\n",
        "Returns:\n",
        "    triplet_loss: scalar, for one batch\n",
        "'''\n",
        "def np_triplet_batch_hard(labels, embedding, margin, soft):\n",
        "    lshape = np.shape(labels)\n",
        "    labels = np.reshape(labels, [lshape[0], 1])\n",
        "    # Build pairwise squared distance matrix.\n",
        "    pdist_matrix = np_distance_metric(embedding, squared=True)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = np.equal(labels, np.transpose(labels)).astype(np.float32)\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = np.logical_not(adjacency).astype(np.float32)\n",
        "\n",
        "    # hard negatives: smallest D_an.\n",
        "    hard_negatives = np_masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "    batch_size = np.size(labels)\n",
        "    mask_positives = adjacency - np.diag(np.ones([batch_size]))\n",
        "    # hard positives: largest D_ap.\n",
        "    hard_positives = np_masked_maximum(pdist_matrix, mask_positives)\n",
        "    if soft:\n",
        "        triplet_loss = np.log1p(np.exp(hard_positives - hard_negatives))\n",
        "    else:\n",
        "        triplet_loss = np.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = np.mean(triplet_loss)\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOlCSy5owj5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7cc5e0ef-405a-4db7-f57a-c294e7f6b780"
      },
      "source": [
        "soft = True\n",
        "tfa_triplet = tfa.losses.TripletHardLoss(0.3, soft)\n",
        "print(tfa_triplet(labels2, emb3))\n",
        "print(np_triplet_batch_hard(labels2,emb3,0.3, soft))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(13.9740715, shape=(), dtype=float32)\n",
            "13.974051951069422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1XdVzcjwlWf",
        "colab_type": "code",
        "outputId": "440a768d-be72-4f23-ee48-96f981009c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "embedding = np.array([[-2., 0., 3.],[-1., 3., 2.],[-3., 1., 6.],[2., -1., -2.]]).astype(np.float32)\n",
        "labels = np.array([1,0,1,0]).astype(np.float32)\n",
        "print(embedding.shape)\n",
        "print(labels.shape)\n",
        "# a = tf.cast(a, dtype=tf.dtypes.float32)\n",
        "squared = True\n",
        "margin = 0.3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 3)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxj0t6M_wtZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "semi-hard batch triplet loss of a batch\n",
        "------------------------------------\n",
        "Args:\n",
        "    labels:     label data, shape = (batch_size,1)\n",
        "    embedding:  embedding vector, shape = (batch_size, vector_size)\n",
        "    margin:     margin, scalar\n",
        "Returns:\n",
        "    triplet_loss: scalar, for one batch\n",
        "'''\n",
        "def np_triplet_batch_semihard(labels, embedding, margin):\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = np.shape(labels)\n",
        "    labels = np.reshape(labels, [lshape[0], 1])\n",
        "    # Build pairwise squared distance matrix.\n",
        "    pdist_matrix = np_distance_metric(embedding, squared=True)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = np.equal(labels, np.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = np.logical_not(adjacency)\n",
        "\n",
        "    batch_size = np.size(labels)\n",
        "    # Compute the mask.\n",
        "    pdist_matrix_tile = np.tile(pdist_matrix, [batch_size, 1])\n",
        "    mask = np.logical_and(np.tile(adjacency_not, [batch_size, 1]),\n",
        "                          np.greater(pdist_matrix_tile, np.reshape(np.transpose(pdist_matrix), [-1, 1])),)\n",
        "    mask_final = np.reshape(np.greater(np.sum(mask.astype(np.float32), 1, keepdims=True),\n",
        "                                       0.0,),\n",
        "                            [batch_size, batch_size],)\n",
        "    mask_final = np.transpose(mask_final)\n",
        "\n",
        "    adjacency_not = adjacency_not.astype(np.float32)\n",
        "    mask = mask.astype(np.float32)\n",
        "\n",
        "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
        "    negatives_outside = np.reshape(np_masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
        "    negatives_outside = np.transpose(negatives_outside)\n",
        "\n",
        "    # negatives_inside: largest D_an.\n",
        "    negatives_inside = np.tile(np_masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
        "\n",
        "    semi_hard_negatives = np.where(mask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "    loss_mat = np.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "\n",
        "    mask_positives = adjacency.astype(np.float32) - np.diag(np.ones([batch_size]))\n",
        "\n",
        "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
        "    #   in semihard, they take all positive pairs except the diagonal.\n",
        "    num_positives = np.sum(mask_positives)\n",
        "\n",
        "    triplet_loss = np.true_divide(np.sum(np.maximum(np.multiply(loss_mat, mask_positives), 0.0)),num_positives,)\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtHABp5NxSc_",
        "colab_type": "code",
        "outputId": "561604cb-a2ce-4db8-d093-a1965e5a624f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# print(embedding.shape)\n",
        "# print(labels.shape)\n",
        "tfa_triplet = tfa.losses.TripletSemiHardLoss(0.3)\n",
        "print(tfa_triplet(labels2, emb3))\n",
        "print(np_triplet_batch_semihard(labels2,emb3,0.3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.15625882, shape=(), dtype=float32)\n",
            "0.1562326908111572\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}