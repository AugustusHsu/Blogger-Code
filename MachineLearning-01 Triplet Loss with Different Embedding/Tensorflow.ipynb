{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_triplet(tensorflow).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4BFJwce09h-",
        "colab_type": "code",
        "outputId": "98416668-c9dc-4deb-df7b-34aad5c694be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "batch = 64\n",
        "emb_dim = 1024\n",
        "\n",
        "np.random.seed(1234)\n",
        "emb1 = np.random.rand(batch,emb_dim).astype(np.float32)\n",
        "np.random.seed(2345)\n",
        "emb2 = np.random.rand(batch,emb_dim).astype(np.float32)\n",
        "emb3 = np.concatenate([emb1, emb2], axis=0)\n",
        "margin = 0.3\n",
        "labels1 = np.arange(batch)\n",
        "labels2 = np.concatenate((labels1,labels1), axis=0)\n",
        "print(emb1.shape)\n",
        "print(emb2.shape)\n",
        "print(emb3.shape)\n",
        "print(labels1.shape)\n",
        "print(labels2.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1024)\n",
            "(64, 1024)\n",
            "(128, 1024)\n",
            "(64,)\n",
            "(128,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frUe90dY1MLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _distance_metric(embedding, squared=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: float32, with shape [m, d], (batch_size, d)\n",
        "        y: float32, with shape [n, d], (batch_size, d)\n",
        "    Returns:\n",
        "        dist: float32, with shape [m, n], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # |x-y|^2 = x^2 - 2xy + y^2\n",
        "    xy = tf.matmul(embedding, tf.transpose(embedding))\n",
        "    square_norm = tf.linalg.diag_part(xy)\n",
        "    xx = tf.expand_dims(square_norm, 0)\n",
        "    yy = tf.expand_dims(square_norm, 1)\n",
        "    distances = tf.math.add(xx, yy) - 2.0 * xy\n",
        "    '''\n",
        "    (batch_size,1)-(batch_size,batch_size): Equivalent to each column operation\n",
        "    (batch_size,batch_size)+(1,batch_size): Equivalent to each row operation\n",
        "    '''\n",
        "\n",
        "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
        "    distances = tf.math.maximum(distances, 0.0)\n",
        "    # Get the mask where the zero distances are at.\n",
        "    error_mask = tf.math.less_equal(distances, 0.0)\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        distances = tf.math.sqrt(distances + tf.cast(error_mask, dtype=tf.dtypes.float32) * 1e-16)\n",
        "\n",
        "    # Undo conditionally adding 1e-16.\n",
        "    distances = tf.math.multiply(distances, tf.cast(tf.math.logical_not(error_mask), dtype=tf.dtypes.float32),)\n",
        "\n",
        "    num_data = tf.shape(embedding)[0]\n",
        "    # Explicitly set diagonals to zero.\n",
        "    mask_offdiagonals = tf.ones_like(distances) - tf.linalg.diag(tf.ones([num_data]))\n",
        "    distances = tf.math.multiply(distances, mask_offdiagonals)\n",
        "\n",
        "    return distances\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29nKYrghA_Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _masked_minimum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "    Args:\n",
        "      data: float32, with shape [n, m], (batch_size, batch_size)\n",
        "      mask: boolean, with shape [n, m], (batch_size, batch_size)\n",
        "      dim: int, the dimension which want to compute the minimum.\n",
        "    Returns:\n",
        "      masked_minimums: float32, with shape [n, 1], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "    masked_minimums = (tf.math.reduce_min(tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True) + axis_maximums)\n",
        "    return masked_minimums\n",
        "def _masked_maximum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
        "    Args:\n",
        "      data: float32, with shape [n, m], (batch_size, batch_size)\n",
        "      mask: boolean, with shape [n, m], (batch_size, batch_size)\n",
        "      dim: int, the dimension over which to compute the maximum.\n",
        "    Returns:\n",
        "      masked_minimums: float32, with shape [n, 1], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n",
        "    masked_maximums = (tf.math.reduce_max(tf.math.multiply(data - axis_minimums, mask), dim, keepdims=True) + axis_minimums)\n",
        "    return masked_maximums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqPy2rcYu4l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "batch hard triplet loss of a batch\n",
        "------------------------------------\n",
        "Args:\n",
        "    labels:     Label Data, shape = (batch_size,1)\n",
        "    embedding:  embedding vector, shape = (batch_size, vector_size)\n",
        "    margin:     margin, scalar\n",
        "    soft::     \tuse log1p or not, boolean\n",
        "Returns:\n",
        "    triplet_loss: scalar, for one batch\n",
        "'''\n",
        "# Reshape label tensor to [batch_size, 1].\n",
        "def triplet_batch_hard(labels, embedding, margin, soft):\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "    # Build pairwise squared distance matrix.\n",
        "    pdist_matrix = _distance_metric(embedding, squared=True)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "\n",
        "    # hard negatives: smallest D_an.\n",
        "    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(tf.ones([batch_size]))\n",
        "    # hard positives: largest D_ap.\n",
        "    hard_positives = _masked_maximum(pdist_matrix, mask_positives)\n",
        "    if soft:\n",
        "        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "    else:\n",
        "        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    return triplet_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqSNpzQ8xanw",
        "colab_type": "code",
        "outputId": "ca715519-ce82-4902-85c6-9835704e0385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "soft = True\n",
        "tfa_triplet = tfa.losses.TripletHardLoss(0.3, soft)\n",
        "print(tfa_triplet(labels2, emb3))\n",
        "print(triplet_batch_hard(labels2,emb3,0.3, soft))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(13.974049, shape=(), dtype=float32)\n",
            "tf.Tensor(13.9740505, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5yLPBaErX3",
        "colab_type": "code",
        "outputId": "43249b27-f033-4b61-e2b6-06b7f6c413a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "embedding = np.array([[-2., 0., 3.],[-1., 3., 2.],[-3., 1., 6.],[2., -1., -2.]]).astype(np.float32)\n",
        "labels = np.array([1,0,1,0]).astype(np.float32)\n",
        "print(embedding.shape)\n",
        "print(labels.shape)\n",
        "# a = tf.cast(a, dtype=tf.dtypes.float32)\n",
        "squared = True\n",
        "margin = 0.3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 3)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGCqY1vVrTlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "semi-hard batch triplet loss of a batch\n",
        "------------------------------------\n",
        "Args:\n",
        "    labels:     label data, shape = (batch_size,1)\n",
        "    embedding:  embedding vector, shape = (batch_size, vector_size)\n",
        "    margin:     margin, scalar\n",
        "Returns:\n",
        "    triplet_loss: scalar, for one batch\n",
        "'''\n",
        "def triplet_batch_semihard(labels, embedding, margin):\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "    # Build pairwise squared distance matrix.\n",
        "    pdist_matrix = _distance_metric(embedding, squared=True)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "    # Compute the mask.\n",
        "    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
        "    mask = tf.math.logical_and(tf.tile(adjacency_not, [batch_size, 1]),\n",
        "                               tf.math.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])),)\n",
        "    mask_final = tf.reshape(tf.math.greater(tf.math.reduce_sum(tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True),\n",
        "                                            0.0,),\n",
        "                            [batch_size, batch_size],)\n",
        "    mask_final = tf.transpose(mask_final)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n",
        "\n",
        "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
        "    negatives_outside = tf.reshape(_masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
        "    negatives_outside = tf.transpose(negatives_outside)\n",
        "\n",
        "    # negatives_inside: largest D_an.\n",
        "    negatives_inside = tf.tile(_masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
        "\n",
        "    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "\n",
        "    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(tf.ones([batch_size]))\n",
        "\n",
        "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
        "    #   in semihard, they take all positive pairs except the diagonal.\n",
        "    num_positives = tf.math.reduce_sum(mask_positives)\n",
        "\n",
        "    triplet_loss = tf.math.truediv(tf.math.reduce_sum(tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)),num_positives,)\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni92v1vX-aAI",
        "colab_type": "code",
        "outputId": "666b4564-9c85-49d0-8624-83bd5aa531b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# print(embedding.shape)\n",
        "# print(labels.shape)\n",
        "tfa_triplet = tfa.losses.TripletSemiHardLoss(0.3)\n",
        "print(tfa_triplet(labels2, emb3))\n",
        "print(triplet_batch_semihard(labels2, emb3, 0.3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.15624666, shape=(), dtype=float32)\n",
            "tf.Tensor(0.156242, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}