{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_triplet(numpy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGGrdX_Owjc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch = 64\n",
        "emb_dim = 1024\n",
        "\n",
        "np.random.seed(1234)\n",
        "emb1 = np.random.rand(batch,emb_dim)\n",
        "np.random.seed(2345)\n",
        "emb2 = np.random.rand(batch,emb_dim)\n",
        "margin = 0.3\n",
        "labels = np.expand_dims(np.concatenate((np.arange(batch/2),np.arange(batch/2)), axis=0), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIsJ9j9NwmJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _distance_metric(x, y):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: tensor, with shape [m, d], (batch_size, d)\n",
        "        y: tensor, with shape [n, d], (batch_size, d)\n",
        "    Returns:\n",
        "        dist: tensor, with shape [m, n], (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # |x-y|^2 = x^2 - 2xy + y^2\n",
        "    # xy\n",
        "    xy = np.matmul(x, np.transpose(y))\n",
        "    # x^2\n",
        "    xx = np.matmul(x, np.transpose(x))\n",
        "    xx = np.diag(xx)\n",
        "    # y^2\n",
        "    yy = np.matmul(y, np.transpose(y))\n",
        "    yy = np.diag(yy)\n",
        "    '''\n",
        "    (batch_size,1)-(batch_size,batch_size):\n",
        "        Equivalent to each column operation\n",
        "    (batch_size,batch_size)+(1,batch_size):\n",
        "        Equivalent to each row operation\n",
        "    '''\n",
        "    distances = np.expand_dims(xx, 1) - 2.0*xy + np.expand_dims(yy, 0)\n",
        "    return distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxWzypKtwqdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _label_mask(labels):\n",
        "    '''\n",
        "    if label is same, label_mask will return True\n",
        "    ------------------------------------\n",
        "    Args:\n",
        "        labels:     Label Data, shape = (batch_size,1)\n",
        "    Returns:\n",
        "        label_mask: tensor, with shape [m, n], (batch_size, batch_size)\n",
        "        ex.\n",
        "            labels = [1,0,1]\n",
        "            label_mask = [[1, 0, 1],\n",
        "                          [0, 1, 0],\n",
        "                          [1, 0, 1]]\n",
        "    '''\n",
        "    label_mask = _distance_metric(labels, labels)\n",
        "    label_mask = (label_mask == 0).astype(np.float32)\n",
        "    return label_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh6lgbGOwq0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_all(labels, emb1, emb2, margin):\n",
        "    '''\n",
        "    batch all triplet loss of a batch\n",
        "    ------------------------------------\n",
        "    Args:\n",
        "        labels:     Label Data, shape = (batch_size,1)\n",
        "        emb1, emb2: Embedding Feature, shape = (batch_size, vector_size)\n",
        "        margin:     margin, scalar\n",
        "    Returns:\n",
        "        triplet_loss: scalar, for one batch\n",
        "    '''\n",
        "    dist_mat = _distance_metric(emb1, emb2)\n",
        "    # an and ap mask\n",
        "    ap_mask = _label_mask(labels)\n",
        "    an_mask = np.logical_not(ap_mask).astype(np.float32)\n",
        "    # distance between anchor and positive\n",
        "    dist_ap = np.sum(dist_mat*ap_mask, axis=1)/np.sum(ap_mask, axis=1)\n",
        "    # ap - dist_mat + margin\n",
        "    mat = np.expand_dims(dist_ap, 1) - dist_mat + margin\n",
        "    # only need ap-an\n",
        "    mat = mat*an_mask\n",
        "    # caluculate the number of valid triplet loss\n",
        "    mask = np.greater(mat, margin).astype(np.float32)\n",
        "    num_valid_triplets = np.sum(mask)\n",
        "    triplet_loss = mat*mask\n",
        "    # <1 : 1\n",
        "    num_valid_triplets = np.maximum(num_valid_triplets, 1.0)\n",
        "    # divided triplet_loss by num_valid_triplets\n",
        "    triplet_loss = np.sum(triplet_loss)/num_valid_triplets\n",
        "    return triplet_loss, num_valid_triplets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxj0t6M_wtZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_loss, num_valid_triplets = batch_all(labels, emb1, emb2, margin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtHABp5NxSc_",
        "colab_type": "code",
        "outputId": "5efc6efa-8880-4fd8-db48-561a8ad19241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "triplet_loss"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.893566522818261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbK5KHdKwzg4",
        "colab_type": "code",
        "outputId": "c489f16e-20f4-411a-a940-6187afefa60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "num_valid_triplets"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1876.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-zN8vYJw0xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_hard(labels, emb1, emb2, margin):\n",
        "    '''\n",
        "    batch all triplet loss of a batch\n",
        "    ------------------------------------\n",
        "    Args:\n",
        "        labels:     Label Data, shape = (batch_size,1)\n",
        "        emb1, emb2: Embedding Feature, shape = (batch_size, vector_size)\n",
        "        margin:     margin, scalar\n",
        "    Returns:\n",
        "        triplet_loss: scalar, for one batch\n",
        "    '''\n",
        "    dist_mat = _distance_metric(emb1, emb2)\n",
        "    # an and ap mask\n",
        "    ap_mask = _label_mask(labels)\n",
        "    an_mask = np.logical_not(ap_mask).astype(np.float32)\n",
        "    # distance between anchor and positive\n",
        "    dist_ap = np.sum(dist_mat*ap_mask, axis=1)/np.sum(ap_mask, axis=1)\n",
        "    # caluculate the number of valid triplet loss\n",
        "    mat = np.expand_dims(dist_ap, 1) - dist_mat + margin\n",
        "    mat = mat*an_mask\n",
        "    mask = np.greater(mat, margin).astype(np.float32)\n",
        "    num_valid_triplets = np.sum(mask)\n",
        "    \n",
        "    # the max of distance between anchor and positive\n",
        "    dist_ap = np.max(dist_mat*ap_mask, axis=1)\n",
        "    # the max of distance between anchor and negative\n",
        "    max_num = np.max(dist_mat)\n",
        "    dist_an = dist_mat*an_mask + ap_mask*max_num\n",
        "    dist_an = np.amin(dist_an, axis=1)\n",
        "    # ap - dist_mat + margin\n",
        "    mat = dist_ap - dist_an + margin\n",
        "    triplet_loss = np.mean(mat)\n",
        "    # <1 : 1\n",
        "    triplet_loss = np.maximum(triplet_loss, 1e-16)\n",
        "    return triplet_loss, num_valid_triplets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7USMQ1NLUv_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_loss, num_valid_triplets = batch_hard(labels, emb1, emb2, margin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8v2tYMMUwi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c852de0d-31ba-4649-da6b-49b630118f6c"
      },
      "source": [
        "triplet_loss"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.3194424177763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTuKYnXpUwcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88318c55-aa53-407c-edc2-0988c193052b"
      },
      "source": [
        "num_valid_triplets"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1876.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}